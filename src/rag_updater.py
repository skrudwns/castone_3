# src/rag_updater.py

import pandas as pd
import re
import emoji
import streamlit as st
import os
from langchain_core.documents import Document
from langchain_huggingface import HuggingFaceEmbeddings # ğŸ‘ˆ [ìˆ˜ì •] ìµœì‹  ê¶Œì¥ ì‚¬í•­
from langchain_community.vectorstores import FAISS
from src.config import review_faiss # config.pyì—ì„œ ê²½ë¡œë§Œ ê°€ì ¸ì˜´

# --- 1. data_process.ipynbì—ì„œ ê°€ì ¸ì˜¨ ì „ì²˜ë¦¬ í•¨ìˆ˜ ---

def clean_review(text):
    """ë¦¬ë·° í…ìŠ¤íŠ¸ë¥¼ ì •ì œí•©ë‹ˆë‹¤."""
    text = str(text) # NaN ë°©ì§€
    text = re.sub(r'\s+', ' ', text)
    text = emoji.replace_emoji(text, replace='')
    text = re.sub(r'[^ê°€-í£a-zA-Z0-9\s]', '', text)
    text = text.strip()
    return text

def chunk_text_with_overlap(text, chunk_size=500, overlap=50):
    """í…ìŠ¤íŠ¸ë¥¼ ì²­í‚¹í•©ë‹ˆë‹¤."""
    text = text.strip()
    if len(text) <= chunk_size:
        return [text]
    
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunks.append(text[start:end])
        start = end - overlap
        if start < 0:
            start = 0
        if start >= len(text):
            break
    return chunks

# --- 2. embedding.ipynbì—ì„œ ê°€ì ¸ì˜¨ ë¬¸ì„œí™” í•¨ìˆ˜ ---

def create_documents_from_df(df):
    """DataFrameì„ LangChain Document ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    docs = []
    for _, row in df.iterrows():
        cleaned_review = clean_review(row["ë¦¬ë·°"])
        
        chunks = chunk_text_with_overlap(cleaned_review, chunk_size=500, overlap=20)
        
        for chunk in chunks:
            if len(chunk) <= 5: # 5ì ì´í•˜ ì²­í¬ëŠ” ë¬´ì‹œ
                continue

            # embedding.ipynbì˜ 'combined_text' ë¡œì§ ì ìš©
            combined_text = (
                f"{row['ì¥ì†Œ']}ì€(ëŠ”) "
                f"{row['ì§€ì—­']}ì— ìœ„ì¹˜í•œ "
                f"{row['ì¹´í…Œê³ ë¦¬_í†µí•©']}ì…ë‹ˆë‹¤. "
                f"ë¦¬ë·° ë‚´ìš©ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: {chunk}"
            )
            
            # embedding.ipynbì˜ Document ìƒì„± ë¡œì§ ì ìš©
            doc = Document(
                page_content=combined_text,
                metadata={
                    "place_name": str(row["ì¥ì†Œ"]),
                    "region": str(row["ì§€ì—­"]),
                    "category": str(row.get("ì¹´í…Œê³ ë¦¬_í†µí•©", "")),
                    "rating": str(row.get("í‰ì ", ""))
                }
            )
            docs.append(doc)
    return docs

# --- 3. ë²¡í„° DB ì—…ë°ì´íŠ¸ í•¨ìˆ˜ (í•µì‹¬) ---

def update_vector_db_if_needed(new_reviews_file="new_reviews.csv"):
    """
    new_reviews.csv íŒŒì¼ì— 10ê°œ ì´ìƒ ë¦¬ë·°ê°€ ìŒ“ì´ë©´
    ë²¡í„° DB(review-faiss)ì— ì¶”ê°€(ì—…ë°ì´íŠ¸)í•©ë‹ˆë‹¤.
    """
    try:
        df = pd.read_csv(new_reviews_file)
    except FileNotFoundError:
        return "ëˆ„ì ëœ ë¦¬ë·° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."
    except pd.errors.EmptyDataError:
        return "ëˆ„ì ëœ ë¦¬ë·°ê°€ ì—†ìŠµë‹ˆë‹¤."

    if len(df) < 10:
        return f"ë¦¬ë·° {len(df)}ê°œ ëˆ„ì ë¨. (10ê°œ ì´ìƒì´ì–´ì•¼ ì—…ë°ì´íŠ¸)"

    st.toast(f"ë¦¬ë·° {len(df)}ê°œê°€ ëˆ„ì ë˜ì–´ ë²¡í„° DB ì—…ë°ì´íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    print(f"--- [RAG Updater] ë¦¬ë·° {len(df)}ê°œ DB ì—…ë°ì´íŠ¸ ì‹œì‘ ---")

    try:
        # 1. ì‹ ê·œ ë¦¬ë·°ë¥¼ Documentë¡œ ë³€í™˜
        new_docs = create_documents_from_df(df)
        if not new_docs:
            print("[RAG Updater] ì²˜ë¦¬í•  ìœ íš¨í•œ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            os.remove(new_reviews_file) # ìœ íš¨í•˜ì§€ ì•Šì€ ë¦¬ë·° íŒŒì¼ ì‚­ì œ
            return "ì—…ë°ì´íŠ¸í•  ìœ íš¨í•œ ë¦¬ë·°ê°€ ì—†ìŠµë‹ˆë‹¤."
            
        print(f"[RAG Updater] {len(new_docs)}ê°œì˜ ìƒˆ ë¬¸ì„œë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")

        # 2. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (embedding.ipynb ì°¸ê³ )
        embeddings = HuggingFaceEmbeddings(
            model_name="upskyy/bge-m3-korean",
            model_kwargs={"device": "cpu"}
        )
        
        # 3. ê¸°ì¡´ FAISS DB ë¡œë“œ
        db = FAISS.load_local(
            review_faiss, embeddings, allow_dangerous_deserialization=True
        )
        print("[RAG Updater] ê¸°ì¡´ FAISS ì¸ë±ìŠ¤ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")

        # 4. DBì— ì‹ ê·œ ë¬¸ì„œ ì¶”ê°€
        db.add_documents(new_docs)
        print("[RAG Updater] FAISS ì¸ë±ìŠ¤ì— ìƒˆ ë¬¸ì„œë¥¼ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.")

        # 5. DB ì €ì¥ (ë®ì–´ì“°ê¸°)
        db.save_local(review_faiss)
        print("[RAG Updater] FAISS ì¸ë±ìŠ¤ë¥¼ ë¡œì»¬ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

        # 6. Streamlit ìºì‹œ ì‚­ì œ (ì¤‘ìš”!)
        # 1_trip_planner.pyê°€ ìƒˆ DBë¥¼ ë¡œë“œí•˜ë„ë¡ ê°•ì œ
        st.cache_resource.clear()
        print("[RAG Updater] Streamlit ìºì‹œë¥¼ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.")

        # 7. ëˆ„ì ëœ ë¦¬ë·° íŒŒì¼ ì‚­ì œ
        os.remove(new_reviews_file)
        print(f"[RAG Updater] {new_reviews_file} íŒŒì¼ì„ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.")
        
        st.toast("ë²¡í„° DB ì—…ë°ì´íŠ¸ ì™„ë£Œ!", icon="ğŸ‰")
        return "ë²¡í„° DB ì—…ë°ì´íŠ¸ ì™„ë£Œ!"

    except Exception as e:
        print(f"!!!!!!!!!! [RAG Updater] ì˜ˆì™¸ ë°œìƒ !!!!!!!!!!")
        print(f"DEBUG: Error details: {e}")
        st.error(f"DB ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return f"ì˜¤ë¥˜: {e}"