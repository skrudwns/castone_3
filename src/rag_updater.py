# src/rag_updater.py

import pandas as pd
import re
import emoji
import streamlit as st
import os
from langchain_core.documents import Document
from langchain_huggingface import HuggingFaceEmbeddings 
from langchain_community.vectorstores import FAISS
from src.config import review_faiss 

# --- 1. ì „ì²˜ë¦¬ í•¨ìˆ˜ ---
def clean_review(text):
    text = str(text) 
    text = re.sub(r'\s+', ' ', text)
    text = emoji.replace_emoji(text, replace='')
    text = re.sub(r'[^ê°€-í£a-zA-Z0-9\s]', '', text)
    text = text.strip()
    return text

def chunk_text_with_overlap(text, chunk_size=500, overlap=50):
    text = text.strip()
    if len(text) <= chunk_size:
        return [text]
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunks.append(text[start:end])
        start = end - overlap
        if start < 0: start = 0
        if start >= len(text): break
    return chunks

# --- 2. [ì‹ ê·œ] ê¸°ì¡´ DBì—ì„œ ì£¼ì†Œ ì°¾ê¸° í—¬í¼ ---
def find_address_from_db(db, place_name):
    """
    ê¸°ì¡´ FAISS DBì—ì„œ ì¥ì†Œëª…ìœ¼ë¡œ ê²€ìƒ‰í•˜ì—¬ 'ìƒì„¸ ì£¼ì†Œ'ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    """
    if not db: return ""
    
    try:
        # ì¥ì†Œëª…ìœ¼ë¡œ ìœ ì‚¬ë„ ê²€ìƒ‰ (ìƒìœ„ 1ê°œë§Œ)
        results = db.similarity_search(place_name, k=1)
        if results:
            doc = results[0]
            # ğŸš¨ [ê²€ì¦] ê²€ìƒ‰ëœ ì¥ì†Œê°€ ë‚´ê°€ ì°¾ëŠ” ì¥ì†Œì™€ ì´ë¦„ì´ ê°™ì€ì§€ í™•ì¸ (ì˜¤ë§¤ì¹­ ë°©ì§€)
            # (ìœ ì‚¬ë„ ê²€ìƒ‰ì´ë¼ 'ì„±ì‹¬ë‹¹' ì°¾ëŠ”ë° 'ì„±ì‹¬ë‹¹ ì¼€ìµë¶€ë ë„'ê°€ ë‚˜ì˜¬ ìˆ˜ ìˆìŒ)
            existing_name = doc.metadata.get("ì¥ì†Œëª…", "")
            
            # ì´ë¦„ì´ ì •í™•íˆ ì¼ì¹˜í•˜ê±°ë‚˜, ê²€ìƒ‰ì–´ê°€ ê²°ê³¼ì— í¬í•¨ë˜ì–´ ìˆë‹¤ë©´ ì‹ ë¢°
            if place_name in existing_name or existing_name in place_name:
                address = doc.metadata.get("ìƒì„¸ ì£¼ì†Œ", "")
                if address:
                    print(f"   [Smart Fill] '{place_name}'ì˜ ì£¼ì†Œë¥¼ DBì—ì„œ ì°¾ì•˜ìŠµë‹ˆë‹¤: {address}")
                    return address
    except Exception as e:
        print(f"DEBUG: ì£¼ì†Œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
    
    return ""

# --- 3. ë¬¸ì„œí™” í•¨ìˆ˜ (ìˆ˜ì •ë¨: DB ì£¼ì†Œ ì¡°íšŒ ë¡œì§ ì¶”ê°€) ---
def create_documents_from_df(df, existing_db=None):
    """
    DataFrame -> Document ë³€í™˜
    * existing_db: ì£¼ì†Œ ì¡°íšŒë¥¼ ìœ„í•´ ì „ë‹¬ë°›ì€ ê¸°ì¡´ FAISS DB ê°ì²´
    """
    docs = []
    for _, row in df.iterrows():
        cleaned_review = clean_review(row.get("ë¦¬ë·°", "")) 
        chunks = chunk_text_with_overlap(cleaned_review, chunk_size=500, overlap=20)
        
        # ì»¬ëŸ¼ ë§¤í•‘
        place_name = row.get("ì¥ì†Œëª…") if pd.notna(row.get("ì¥ì†Œëª…")) else row.get("ì¥ì†Œ", "ì¥ì†Œë¯¸ìƒ")
        category = row.get("ì¹´í…Œê³ ë¦¬_í†µí•©") if pd.notna(row.get("ì¹´í…Œê³ ë¦¬_í†µí•©")) else row.get("ì¹´í…Œê³ ë¦¬", "ê¸°íƒ€")
        rating = row.get("í‰ì ") if pd.notna(row.get("í‰ì ")) else row.get("ë³„ì ", "0")
        
        # ğŸš¨ [í•µì‹¬ ë¡œì§] ì£¼ì†Œ ì±„ìš°ê¸° ì „ëµ
        # 1. ì…ë ¥ëœ ì£¼ì†Œê°€ ìˆìœ¼ë©´ ê·¸ê±° ì”€
        # 2. ì—†ìœ¼ë©´ DBì—ì„œ ì°¾ì•„ë´„
        # 3. ê·¸ë˜ë„ ì—†ìœ¼ë©´ ë¹ˆ ê°’("")
        address = row.get("ìƒì„¸ ì£¼ì†Œ") if pd.notna(row.get("ìƒì„¸ ì£¼ì†Œ")) else ""
        
        if not address and existing_db:
            address = find_address_from_db(existing_db, place_name)

        for chunk in chunks:
            if len(chunk) <= 5: continue

            combined_text = (
                f"ì§€ì—­: {row.get('ì§€ì—­', '')} | "
                f"ì¥ì†Œëª…: {place_name} | "
                f"ì¹´í…Œê³ ë¦¬: {category} | "
                f"ë¦¬ë·°: {chunk}"
            )
            
            doc = Document(
                page_content=combined_text,
                metadata={
                    "ì§€ì—­": str(row.get("ì§€ì—­", "")),
                    "ì¹´í…Œê³ ë¦¬": str(category),
                    "ì¥ì†Œëª…": str(place_name),
                    "ë³„ì ": str(rating),
                    "ìƒì„¸ ì£¼ì†Œ": str(address),  # ì°¾ì•„ë‚¸ ì£¼ì†Œê°€ ë“¤ì–´ê°
                    "ë¦¬ë·°": str(row.get("ë¦¬ë·°", "")[:100])
                }
            )
            docs.append(doc)
    return docs

# --- 4. ë²¡í„° DB ì—…ë°ì´íŠ¸ í•¨ìˆ˜ (ìˆ˜ì •ë¨: DB ë¨¼ì € ë¡œë“œ) ---
def update_vector_db_if_needed(new_reviews_file="new_reviews.csv"):
    try:
        df = pd.read_csv(new_reviews_file)
    except (FileNotFoundError, pd.errors.EmptyDataError):
        return "ì—…ë°ì´íŠ¸í•  ë¦¬ë·°ê°€ ì—†ìŠµë‹ˆë‹¤."

    if len(df) < 10:
        return f"ë¦¬ë·° {len(df)}ê°œ ëˆ„ì ë¨. (10ê°œ ì´ìƒì´ì–´ì•¼ ì—…ë°ì´íŠ¸)"

    st.toast(f"ë¦¬ë·° {len(df)}ê°œ DB ì—…ë°ì´íŠ¸ ì‹œì‘...")
    print(f"--- [RAG Updater] ë¦¬ë·° {len(df)}ê°œ DB ì—…ë°ì´íŠ¸ ì‹œì‘ ---")

    try:
        # 1. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
        embeddings = HuggingFaceEmbeddings(
            model_name="upskyy/bge-m3-korean",
            model_kwargs={"device": "cpu"}
        )
        
        # 2. [ìˆœì„œ ë³€ê²½] ê¸°ì¡´ DBë¥¼ ë¨¼ì € ë¡œë“œ (ê²€ìƒ‰ìš©)
        existing_db = None
        if os.path.exists(review_faiss):
            try:
                existing_db = FAISS.load_local(
                    review_faiss, embeddings, allow_dangerous_deserialization=True
                )
                print("[RAG Updater] ê¸°ì¡´ DB ë¡œë“œ ì™„ë£Œ (ì£¼ì†Œ ê²€ìƒ‰ìš©)")
            except Exception as e:
                print(f"[RAG Updater] ê¸°ì¡´ DB ë¡œë“œ ì‹¤íŒ¨: {e}")

        # 3. ë¬¸ì„œ ìƒì„± (ì—¬ê¸°ì„œ existing_dbë¥¼ ë„˜ê²¨ì¤˜ì„œ ì£¼ì†Œë¥¼ ì°¾ê²Œ í•¨)
        new_docs = create_documents_from_df(df, existing_db=existing_db)
        
        if not new_docs:
            os.remove(new_reviews_file) 
            return "ìœ íš¨í•œ ë¬¸ì„œ ì—†ìŒ"

        print(f"[RAG Updater] {len(new_docs)}ê°œì˜ ìƒˆ ë¬¸ì„œ ìƒì„± ì™„ë£Œ")

        # 4. DBì— ì¶”ê°€ (existing_dbê°€ ìˆìœ¼ë©´ ê±°ê¸°ì— ì¶”ê°€, ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±)
        if existing_db:
            existing_db.add_documents(new_docs)
            db_to_save = existing_db
        else:
            print("[RAG Updater] ê¸°ì¡´ DBê°€ ì—†ì–´ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
            db_to_save = FAISS.from_documents(new_docs, embeddings)

        # 5. ì €ì¥ ë° ì •ë¦¬
        db_to_save.save_local(review_faiss)
        st.cache_resource.clear()
        os.remove(new_reviews_file)
        
        print("[RAG Updater] ì—…ë°ì´íŠ¸ ì™„ë£Œ ë° ì €ì¥ë¨.")
        st.toast("ë²¡í„° DB ì—…ë°ì´íŠ¸ ì™„ë£Œ!", icon="ğŸ‰")
        return "ë²¡í„° DB ì—…ë°ì´íŠ¸ ì™„ë£Œ!"

    except Exception as e:
        print(f"DEBUG: Critical Error: {e}")
        return f"ì˜¤ë¥˜: {e}"