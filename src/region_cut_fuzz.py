# src/region_cut_fuzz.py

import re
from typing import List, Set, Dict
from langchain_core.documents import Document

# í‘œì¤€ ì§€ì—­ëª… ë¦¬ìŠ¤íŠ¸
CANON: List[str] = [
    "ì„œìš¸","ë¶€ì‚°","ëŒ€êµ¬","ì¸ì²œ","ê´‘ì£¼","ëŒ€ì „","ìš¸ì‚°","ì„¸ì¢…",
    "ê²½ê¸°","ê°•ì›","ì¶©ë¶","ì¶©ë‚¨","ì „ë¶","ì „ë‚¨","ê²½ë¶","ê²½ë‚¨","ì œì£¼"
]

# í”í•œ ë³„ì¹­/ì˜ë¬¸/ì˜¤íƒ€ ë§¤í•‘
ALIASES: Dict[str, Set[str]] = {
    "ì„œìš¸": {"ì„œìš¸ì‹œ","ì„œìš¸íŠ¹ë³„ì‹œ","seoul","ì„œìš¸íŠ¹"},
    "ë¶€ì‚°": {"ë¶€ì‚°ì‹œ","ë¶€ì‚°ê´‘ì—­ì‹œ","busan"},
    "ëŒ€êµ¬": {"ëŒ€êµ¬ì‹œ","ëŒ€êµ¬ê´‘ì—­ì‹œ","daegu"},
    "ì¸ì²œ": {"ì¸ì²œì‹œ","ì¸ì²œê´‘ì—­ì‹œ","incheon"},
    "ê´‘ì£¼": {"ê´‘ì£¼ì‹œ","ê´‘ì£¼ê´‘ì—­ì‹œ","gwangju"},
    "ëŒ€ì „": {"ëŒ€ì „ì‹œ","ëŒ€ì „ê´‘ì—­ì‹œ","daejeon"},
    "ìš¸ì‚°": {"ìš¸ì‚°ì‹œ","ìš¸ì‚°ê´‘ì—­ì‹œ","ulsan"},
    "ì„¸ì¢…": {"ì„¸ì¢…ì‹œ","ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ","sejong"},
    "ê²½ê¸°": {"ê²½ê¸°ë„","gyeonggi"},
    "ê°•ì›": {"ê°•ì›ë„","gangwon","ê°•ì›íŠ¹ë³„ìì¹˜ë„"},
    "ì¶©ë¶": {"ì¶©ì²­ë¶ë„","chungbuk"},
    "ì¶©ë‚¨": {"ì¶©ì²­ë‚¨ë„","chungnam"},
    "ì „ë¶": {"ì „ë¼ë¶ë„","jeonbuk","ì „ë¶íŠ¹ë³„ìì¹˜ë„"},
    "ì „ë‚¨": {"ì „ë¼ë‚¨ë„","jeonnam"},
    "ê²½ë¶": {"ê²½ìƒë¶ë„","gyeongbuk"},
    "ê²½ë‚¨": {"ê²½ìƒë‚¨ë„","gyeongnam"},
    "ì œì£¼": {"ì œì£¼ë„","jeju","ì œì£¼íŠ¹ë³„ìì¹˜ë„"},
}

# ê¶Œì—­/ì§‘í•© í† í° í™•ì¥
MACROS: Dict[str, Set[str]] = {
    "ìˆ˜ë„ê¶Œ": {"ì„œìš¸","ê²½ê¸°","ì¸ì²œ"},
    "ë¶€ìš¸ê²½": {"ë¶€ì‚°","ìš¸ì‚°","ê²½ë‚¨"},
    "ì˜ë‚¨": {"ë¶€ì‚°","ëŒ€êµ¬","ìš¸ì‚°","ê²½ë¶","ê²½ë‚¨"},
    "í˜¸ë‚¨": {"ê´‘ì£¼","ì „ë¶","ì „ë‚¨"},
    "ì¶©ì²­ê¶Œ": {"ëŒ€ì „","ì„¸ì¢…","ì¶©ë¶","ì¶©ë‚¨"},
    "ê°•ì›ê¶Œ": {"ê°•ì›"},
    "ì œì£¼ê¶Œ": {"ì œì£¼"},
    "ì„œìš¸ê·¼êµ": {"ì„œìš¸","ê²½ê¸°","ì¸ì²œ"},
    "ìˆ˜ë„": {"ì„œìš¸"},
}

SEP = re.compile(r"[,\|/Â·\-]")

def _tokenize_query(q: str) -> List[str]:
    q = SEP.sub(" ", q)
    q = re.sub(r"\s+", " ", q).strip()
    return q.split()

# ğŸš¨ [ì¶”ê°€ëœ í•¨ìˆ˜] ì§€ì—­ëª… ì •ê·œí™” í•¨ìˆ˜
def normalize_region_name(query: str) -> str:
    """
    ì…ë ¥ëœ ì§€ì—­ëª…(ì˜ˆ: 'ë¶€ì‚°ê´‘ì—­ì‹œ', 'Busan')ì„ í‘œì¤€ ëª…ì¹­(ì˜ˆ: 'ë¶€ì‚°')ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    ë§¤ì¹­ë˜ëŠ” ê²ƒì´ ì—†ìœ¼ë©´ ì›ë³¸ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    q = query.strip()
    
    # 1. CANON ë¦¬ìŠ¤íŠ¸ì— ìˆëŠ”ì§€ í™•ì¸ (ì´ë¯¸ í‘œì¤€ì„)
    if q in CANON:
        return q
        
    # 2. ALIASES ë”•ì…”ë„ˆë¦¬ í™•ì¸ (ë³„ì¹­ -> í‘œì¤€)
    for canon, alias_set in ALIASES.items():
        if q in alias_set:
            return canon
            
    # 3. ì ‘ë¯¸ì‚¬ ì œê±° í›„ í™•ì¸ (ë‹¨ìˆœ ë§¤ì¹­)
    simple_name = q.replace("íŠ¹ë³„ì‹œ","").replace("ê´‘ì—­ì‹œ","").replace("íŠ¹ë³„ìì¹˜ì‹œ","").replace("íŠ¹ë³„ìì¹˜ë„","").replace("ë„","").replace("ì‹œ","")
    if simple_name in CANON:
        return simple_name
        
    return q

def parse_regions_from_query(query: str, fuzzy: bool = True, fuzzy_threshold: int = 85) -> Set[str]:
    """ì¿¼ë¦¬ì—ì„œ ê´‘ì—­ì‹œ/ë„ ì§‘í•©ì„ ë½‘ëŠ”ë‹¤. (ì •í™•ì¼ì¹˜ â†’ ë³„ì¹­/ê¶Œì—­ â†’ í¼ì§€ë§¤ì¹­ ìˆœ)"""
    q = query.lower()
    tokens = _tokenize_query(q)

    found: Set[str] = set()

    # 1) ì •í™• ì¼ì¹˜
    for c in CANON:
        if c in query:
            found.add(c)

    # 2) ê¶Œì—­/ë§¤í¬ë¡œ
    for macro, expands in MACROS.items():
        if macro in query:
            found |= expands

    # 3) ë³„ì¹­ ì¼ì¹˜
    for canon, alset in ALIASES.items():
        if any(a.lower() in q for a in alset):
            found.add(canon)

    # 4) í¼ì§€ ë§¤ì¹­
    if fuzzy:
        try:
            from rapidfuzz import process, fuzz
            for t in tokens:
                if len(t) < 2: 
                    continue
                cand, score, _ = process.extractOne(
                    t, CANON, scorer=fuzz.WRatio
                )
                if score >= fuzzy_threshold:
                    found.add(cand)
        except Exception:
            pass

    return found

def filter_docs_by_region(docs: List[Document], allowed: Set[str], field: str = "ì§€ì—­",
                          drop_unknown: bool = True) -> List[Document]:
    """ë¦¬íŠ¸ë¦¬ë²„ ê²°ê³¼ë¥¼ ê´‘ì—­ì‹œ/ë„ë¡œ ì»·."""
    if not allowed:
        return docs
    out = []
    for d in docs:
        reg = str((d.metadata or {}).get(field, ""))
        
        is_match = False
        for target in allowed:
            if target in reg:
                is_match = True
                break
        
        if is_match:
            out.append(d)
        elif not reg or reg == "nan":
            if not drop_unknown:
                out.append(d)
    return out